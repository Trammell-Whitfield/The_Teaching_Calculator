# ðŸ“ Holy Calculator - File Location Guide

**Project Location**: `/Users/elhoyabembe/Documents/GitHub/Holy-calc-pi/`

## ðŸŽ¯ **THE MOST IMPORTANT FILES** (for Raspberry Pi)

**RECOMMENDED MODEL** (Best Performance):
```
/Users/elhoyabembe/Documents/GitHub/Holy-calc-pi/models/quantized/qwen2.5-math-7b-instruct-q5km.gguf
```
**Size**: 5.1 GB
**Purpose**: Qwen2.5-Math Q5_K_M quantized model - Superior accuracy for mathematical reasoning
**Status**: âœ… READY TO USE (PREFERRED)

**BACKUP MODEL** (Smaller, Faster):
```
/Users/elhoyabembe/Documents/GitHub/Holy-calc-pi/models/quantized/qwen2.5-math-7b-instruct-q4km.gguf
```
**Size**: 4.4 GB
**Purpose**: Qwen2.5-Math Q4_K_M quantized - Good balance of speed and accuracy
**Status**: âœ… READY TO USE

---

## ðŸ“‚ Complete Directory Structure

```
/Users/elhoyabembe/Documents/GitHub/Holy-calc-pi/
â”‚
â”œâ”€â”€ ðŸ“‹ README.md                    # Project overview
â”œâ”€â”€ ðŸ“‹ WHERE_IS_EVERYTHING.md       # This file!
â”œâ”€â”€ ðŸ“‹ requirements.txt             # Python dependencies
â”œâ”€â”€ ðŸ“‹ requirements_mqtt.txt        # MQTT/IoT dependencies
â”œâ”€â”€ ðŸ“‹ .gitignore                   # Git ignore rules
â”œâ”€â”€ ðŸ“‹ main.py                      # Main calculator application (10KB)
â”‚
â”œâ”€â”€ ðŸ“ models/                      # AI Models (~86GB total)
â”‚   â”œâ”€â”€ base/                       # Original downloaded models (HuggingFace format)
â”‚   â”‚   â”œâ”€â”€ deepseek-math-7b-instruct/     # DeepSeek-Math base model
â”‚   â”‚   â””â”€â”€ Qwen2.5-Math-7B-Instruct/      # Qwen2.5-Math base model
â”‚   â”‚
â”‚   â””â”€â”€ quantized/                  # Quantized GGUF models for deployment
â”‚       â”œâ”€â”€ deepseek-math-7b-f16.gguf      # 13 GB (full precision)
â”‚       â”œâ”€â”€ deepseek-math-7b-q4km.gguf     # 3.9 GB
â”‚       â”œâ”€â”€ deepseek-math-7b-q5km.gguf     # 4.6 GB
â”‚       â”œâ”€â”€ qwen2.5-math-7b-instruct-f16.gguf   # 14 GB (full precision)
â”‚       â”œâ”€â”€ qwen2.5-math-7b-instruct-q4km.gguf  # 4.4 GB
â”‚       â””â”€â”€ qwen2.5-math-7b-instruct-q5km.gguf  # 5.1 GB â­ BEST FOR PI!
â”‚
â”œâ”€â”€ ðŸ“ llama.cpp/                   # LLM Inference Engine (~478MB)
â”‚   â”œâ”€â”€ build/bin/
â”‚   â”‚   â”œâ”€â”€ llama-cli               # Run LLM inference
â”‚   â”‚   â”œâ”€â”€ llama-quantize          # Quantize models
â”‚   â”‚   â”œâ”€â”€ llama-bench             # Benchmark performance
â”‚   â”‚   â””â”€â”€ llama-server            # API server
â”‚   â”œâ”€â”€ convert_hf_to_gguf.py       # PyTorch â†’ GGUF converter
â”‚   â””â”€â”€ [source code]
â”‚
â”œâ”€â”€ ðŸ“ scripts/                     # Python Scripts (~612KB)
â”‚   â”œâ”€â”€ download_model.py           # Downloads models from HuggingFace
â”‚   â”œâ”€â”€ compare_llm_models.py       # LLM model comparison tool
â”‚   â”œâ”€â”€ setup_mqtt.sh               # MQTT broker setup
â”‚   â”œâ”€â”€ validate_dependencies.sh    # Dependency checker
â”‚   â”œâ”€â”€ cascade/                    # Math solving cascade
â”‚   â”‚   â”œâ”€â”€ cascade_orchestrator.py # Main orchestrator
â”‚   â”‚   â”œâ”€â”€ llm_handler.py          # LLM integration
â”‚   â”‚   â”œâ”€â”€ sympy_handler.py        # SymPy solver
â”‚   â”‚   â””â”€â”€ wolfram_handler.py      # Wolfram Alpha API
â”‚   â”œâ”€â”€ cache/                      # Query caching system
â”‚   â”‚   â””â”€â”€ query_cache.py          # Cache implementation
â”‚   â”œâ”€â”€ hardware/                   # TI-84 integration
â”‚   â”‚   â”œâ”€â”€ ti84_interface.py       # TI-84 serial interface
â”‚   â”‚   â””â”€â”€ ti84_protocol.py        # TI-84 protocol handler
â”‚   â”œâ”€â”€ monitoring/                 # Performance monitoring
â”‚   â”‚   â””â”€â”€ [monitoring scripts]
â”‚   â””â”€â”€ testing/                    # Test suites
â”‚       â””â”€â”€ [test files]
â”‚
â”œâ”€â”€ ðŸ“ data/                        # Test Data & Configuration
â”‚   â”œâ”€â”€ test-cases/                 # Test problem sets
â”‚   â””â”€â”€ config/                     # Configuration files
â”‚
â”œâ”€â”€ ðŸ“ cache/                       # Query Cache Storage
â”‚   â””â”€â”€ [cached responses]
â”‚
â”œâ”€â”€ ðŸ“ docs/                        # Documentation (~348KB)
â”‚   â”œâ”€â”€ mac-system-baseline.txt     # Your Mac specs
â”‚   â”œâ”€â”€ phase0-completion-summary.md
â”‚   â”œâ”€â”€ phase1-summary.md
â”‚   â”œâ”€â”€ phase2-completion-summary.md
â”‚   â”œâ”€â”€ offline-pi-deployment-plan.md
â”‚   â”œâ”€â”€ project-status-summary.md
â”‚   â””â”€â”€ [25+ other documentation files]
â”‚
â”œâ”€â”€ ðŸ“ logs/                        # Build & Test Logs (~156KB)
â”‚   â”œâ”€â”€ model-download.log          # Download log
â”‚   â”œâ”€â”€ gguf-conversion.log         # GGUF conversion log
â”‚   â”œâ”€â”€ quantize-q4km.log           # Quantization logs
â”‚   â””â”€â”€ [other logs]
â”‚
â”œâ”€â”€ ðŸ“ pi-stats-app/                # Raspberry Pi monitoring app (~144KB)
â”‚   â”œâ”€â”€ app.py                      # Flask web dashboard
â”‚   â””â”€â”€ [frontend files]
â”‚
â”œâ”€â”€ ðŸ“ venv/                        # Python Virtual Environment (~162MB)
â”‚   â”œâ”€â”€ bin/python3                 # Python interpreter
â”‚   â””â”€â”€ lib/python3.x/site-packages/
â”‚       â”œâ”€â”€ sympy/                  # Symbolic mathematics
â”‚       â”œâ”€â”€ numpy/                  # Numerical computing
â”‚       â””â”€â”€ [other packages]
â”‚
â””â”€â”€ ðŸ“ CALC_env/                    # Alternative virtual environment (~874MB)
    â””â”€â”€ [conda/alternative env]
```

---

## ðŸš€ How to Use This Project

### On Mac (Development)

**Activate Python environment:**
```bash
cd /Users/elhoyabembe/Documents/GitHub/Holy-calc-pi
source venv/bin/activate
```

**Test LLM locally (Qwen2.5-Math - RECOMMENDED):**
```bash
cd llama.cpp
./build/bin/llama-cli \
  -m ../models/quantized/qwen2.5-math-7b-instruct-q5km.gguf \
  -p "Solve for x: 2x + 5 = 13" \
  -n 100
```

**Run main calculator:**
```bash
python3 main.py
```

**Compare LLM models:**
```bash
python3 scripts/compare_llm_models.py
```

### Transfer to Raspberry Pi

**What to copy:**
1. **ESSENTIAL**: `models/quantized/qwen2.5-math-7b-instruct-q5km.gguf` (5.1 GB) â­
2. **BACKUP**: `models/quantized/qwen2.5-math-7b-instruct-q4km.gguf` (4.4 GB)
3. `main.py` - Main calculator application
4. `scripts/` folder - All Python code
5. `requirements.txt` & `requirements_mqtt.txt` - Dependencies
6. llama.cpp binaries (rebuild on Pi or cross-compile)

**Transfer methods:**
```bash
# Option 1: USB Drive
cp models/quantized/qwen2.5-math-7b-instruct-q5km.gguf /Volumes/USB_DRIVE/

# Option 2: SCP (one-time WiFi)
scp models/quantized/qwen2.5-math-7b-instruct-q5km.gguf pi@raspberrypi.local:~/

# Option 3: SD Card (before booting Pi)
# Mount SD card and copy directly to /boot or /home partition
```

**Quick deployment script:**
```bash
# See DEPLOY_CHEATSHEET.md and MODEL_TRANSFER_GUIDE.md for detailed instructions
```

---

## ðŸ“Š Storage Breakdown

| Location | Size | Keep on Mac? | Transfer to Pi? |
|----------|------|--------------|-----------------|
| `models/base/deepseek-math-7b-instruct/` | ~13 GB | âœ… Yes | âŒ No |
| `models/base/Qwen2.5-Math-7B-Instruct/` | ~14 GB | âœ… Yes | âŒ No |
| `models/quantized/deepseek-math-7b-f16.gguf` | 13 GB | âš ï¸ Optional | âŒ No |
| `models/quantized/deepseek-math-7b-q4km.gguf` | 3.9 GB | âœ… Yes | âš ï¸ Optional |
| `models/quantized/deepseek-math-7b-q5km.gguf` | 4.6 GB | âœ… Yes | âš ï¸ Optional |
| `models/quantized/qwen2.5-math-7b-instruct-f16.gguf` | 14 GB | âš ï¸ Optional | âŒ No |
| `models/quantized/qwen2.5-math-7b-instruct-q4km.gguf` | 4.4 GB | âœ… Yes | âœ… Backup |
| `models/quantized/qwen2.5-math-7b-instruct-q5km.gguf` | 5.1 GB | âœ… Yes | âœ… **PRIMARY!** |
| `llama.cpp/` | 478 MB | âœ… Yes | âœ… Yes (rebuild) |
| `scripts/` | 612 KB | âœ… Yes | âœ… Yes |
| `main.py` | 10 KB | âœ… Yes | âœ… Yes |
| `venv/` | 162 MB | âœ… Yes | âŒ No (rebuild on Pi) |
| `CALC_env/` | 874 MB | âš ï¸ Optional | âŒ No |
| **TOTAL** | ~86 GB | | **~5-10 GB** |

---

## ðŸ”‘ Key Files Explained

### For Development (Mac)
- `main.py` - Main calculator orchestrator with cascade logic
- `llama.cpp/build/bin/llama-cli` - Test LLM inference
- `scripts/compare_llm_models.py` - Compare model performance
- `scripts/download_model.py` - Download models from HuggingFace
- `requirements.txt` - Python packages (`pip install -r requirements.txt`)
- `venv/` - Isolated Python environment

### For Deployment (Pi)
- `models/quantized/qwen2.5-math-7b-instruct-q5km.gguf` - **PRIMARY** model (best accuracy)
- `models/quantized/qwen2.5-math-7b-instruct-q4km.gguf` - Backup model (faster)
- `main.py` - Main calculator application
- `scripts/cascade/cascade_orchestrator.py` - SymPy â†’ Wolfram â†’ LLM cascade logic
- `scripts/cascade/sympy_handler.py` - Symbolic math solver
- `scripts/cascade/llm_handler.py` - LLM integration
- `scripts/hardware/ti84_interface.py` - TI-84 serial interface
- `scripts/cache/query_cache.py` - Response caching system

### Documentation
- `WHERE_IS_EVERYTHING.md` - This file (complete project map)
- `DEPLOY_CHEATSHEET.md` - Quick deployment reference
- `MODEL_TRANSFER_GUIDE.md` - How to transfer models to Pi
- `QUICK_REFERENCE.md` - Common commands and operations
- `QUICKSTART_PI.md` - Raspberry Pi setup guide
- `docs/offline-pi-deployment-plan.md` - Offline deployment strategy
- `docs/project-status-summary.md` - Current project status

---

## ðŸŽ“ Git Repository

Your project is a Git repository:

```bash
cd /Users/elhoyabembe/Documents/GitHub/Holy-calc-pi
git status
git log
```

**Recent commits:**
- Switch to Qwen2.5-Math models with Q5_K_M preferred quantization
- Create validate_dependencies.sh
- Create compare_llm_models.py
- Create setup_mqtt.sh
- Multiple phase completions and documentation updates

---

## âš¡ Quick Commands

### See what's taking up space:
```bash
cd /Users/elhoyabembe/Documents/GitHub/Holy-calc-pi
du -sh *
```

### Check model file sizes:
```bash
ls -lh models/quantized/
```

### Test if LLM works:
```bash
cd llama.cpp
./build/bin/llama-cli --version
./build/bin/llama-cli -m ../models/quantized/qwen2.5-math-7b-instruct-q5km.gguf -p "2+2" -n 50
```

### Update Python requirements:
```bash
source venv/bin/activate
pip freeze > requirements.txt
```

### Run model comparison:
```bash
python3 scripts/compare_llm_models.py
```

### Validate dependencies before Pi transfer:
```bash
bash scripts/validate_dependencies.sh
```

---

## ðŸŒŸ Project Status & Next Steps

### âœ… Completed
- Phase 0: Environment setup (Mac + Python + llama.cpp)
- Phase 1: Model download (both DeepSeek-Math and Qwen2.5-Math)
- Phase 2: Model quantization (Q4_K_M and Q5_K_M variants)
- Phase 3: Model comparison testing (Qwen2.5 Q5_K_M is best)
- Phase 4: Cascade orchestrator (SymPy â†’ Wolfram â†’ LLM)
- Phase 5: Query caching system
- Phase 6: TI-84 interface implementation
- Phase 7: MQTT integration for IoT
- Phase 8: Pi monitoring dashboard

### ðŸ”„ In Progress
- Final testing and optimization
- Documentation refinement

### ðŸ“‹ Ready for Deployment
- Transfer Qwen2.5-Math Q5_K_M model to Raspberry Pi
- Set up offline calculator on Pi
- Test complete cascade system on Pi hardware

### ðŸŽ¯ Optional Enhancements
- ESP32 integration for wireless display
- Enhanced caching strategies
- Additional model fine-tuning

---

**Last Updated**: December 17, 2025
**Project Status**: READY FOR RASPBERRY PI DEPLOYMENT
**Primary Model**: Qwen2.5-Math-7B Q5_K_M (5.1 GB)
**Total Project Size**: ~86 GB (Mac) â†’ ~5-10 GB (Pi transfer)
