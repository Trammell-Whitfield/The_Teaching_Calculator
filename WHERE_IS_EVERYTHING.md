# ðŸ“ Holy Calculator - File Location Guide

**Project Location**: `/Users/elhoyabembe/Documents/GitHub/Holy_Calculator/`

## ðŸŽ¯ **THE MOST IMPORTANT FILE** (for Raspberry Pi)

This is what you'll transfer to your Pi:

```
/Users/elhoyabembe/Documents/GitHub/Holy_Calculator/models/quantized/deepseek-math-7b-q4km.gguf
```
**Size**: 3.9 GB
**Purpose**: Quantized LLM model for offline mathematical reasoning
**Status**: âœ… READY TO USE

---

## ðŸ“‚ Complete Directory Structure

```
/Users/elhoyabembe/Documents/GitHub/Holy_Calculator/
â”‚
â”œâ”€â”€ ðŸ“‹ README.md                    # Project overview
â”œâ”€â”€ ðŸ“‹ WHERE_IS_EVERYTHING.md       # This file!
â”œâ”€â”€ ðŸ“‹ requirements.txt             # Python dependencies
â”œâ”€â”€ ðŸ“‹ .gitignore                   # Git ignore rules
â”‚
â”œâ”€â”€ ðŸ“ models/                      # AI Models (26GB total)
â”‚   â”œâ”€â”€ base/                       # Original downloaded model
â”‚   â”‚   â””â”€â”€ deepseek-math-7b-instruct/    # 12.9 GB (PyTorch format)
â”‚   â”‚       â”œâ”€â”€ pytorch_model-00001-of-00002.bin  (9.3GB)
â”‚   â”‚       â”œâ”€â”€ pytorch_model-00002-of-00002.bin  (3.6GB)
â”‚   â”‚       â”œâ”€â”€ config.json
â”‚   â”‚       â”œâ”€â”€ tokenizer.json
â”‚   â”‚       â””â”€â”€ [other config files]
â”‚   â”‚
â”‚   â””â”€â”€ quantized/                  # Quantized models for deployment
â”‚       â”œâ”€â”€ deepseek-math-7b-f16.gguf      # 13 GB (intermediate)
â”‚       â”œâ”€â”€ deepseek-math-7b-q4km.gguf     # 3.9 GB â­ USE THIS ON PI!
â”‚       â””â”€â”€ deepseek-math-7b-q5km.gguf     # ~4.8 GB (backup, creating...)
â”‚
â”œâ”€â”€ ðŸ“ llama.cpp/                   # LLM Inference Engine
â”‚   â”œâ”€â”€ build/bin/
â”‚   â”‚   â”œâ”€â”€ llama-cli               # Run LLM inference
â”‚   â”‚   â”œâ”€â”€ llama-quantize          # Quantize models
â”‚   â”‚   â”œâ”€â”€ llama-bench             # Benchmark performance
â”‚   â”‚   â””â”€â”€ llama-server            # API server
â”‚   â”œâ”€â”€ convert_hf_to_gguf.py       # PyTorch â†’ GGUF converter
â”‚   â””â”€â”€ [source code]
â”‚
â”œâ”€â”€ ðŸ“ scripts/                     # Python Scripts
â”‚   â”œâ”€â”€ download_model.py           # Downloads models from HuggingFace
â”‚   â”œâ”€â”€ cascade/                    # (Empty - Phase 7)
â”‚   â”‚   â””â”€â”€ [future: SymPy, Wolfram, LLM handlers]
â”‚   â”œâ”€â”€ monitoring/                 # (Empty - Phase 3)
â”‚   â”‚   â””â”€â”€ [future: performance monitoring]
â”‚   â””â”€â”€ testing/                    # (Empty - Phase 4)
â”‚       â””â”€â”€ [future: test suites]
â”‚
â”œâ”€â”€ ðŸ“ data/                        # Test Data
â”‚   â””â”€â”€ test-cases/                 # (Empty - Phase 3)
â”‚       â””â”€â”€ [future: math-problems.yaml]
â”‚
â”œâ”€â”€ ðŸ“ docs/                        # Documentation
â”‚   â”œâ”€â”€ mac-system-baseline.txt     # Your Mac specs
â”‚   â”œâ”€â”€ llamacpp-version.txt        # llama.cpp version
â”‚   â”œâ”€â”€ build-config.txt            # Build configuration
â”‚   â”œâ”€â”€ base-model-manifest.txt     # List of model files
â”‚   â”œâ”€â”€ phase0-completion-summary.md
â”‚   â”œâ”€â”€ phase1-summary.md
â”‚   â”œâ”€â”€ offline-pi-deployment-plan.md  # â­ Important!
â”‚   â”œâ”€â”€ project-status-summary.md
â”‚   â””â”€â”€ WHERE_IS_EVERYTHING.md      # This file
â”‚
â”œâ”€â”€ ðŸ“ logs/                        # Build & Test Logs
â”‚   â”œâ”€â”€ model-download.log          # Download log
â”‚   â”œâ”€â”€ gguf-conversion.log         # GGUF conversion log
â”‚   â”œâ”€â”€ quantize-q4km.log           # Q4 quantization log
â”‚   â””â”€â”€ quantize-q5km.log           # Q5 quantization log (creating...)
â”‚
â””â”€â”€ ðŸ“ venv/                        # Python Virtual Environment
    â”œâ”€â”€ bin/
    â”‚   â””â”€â”€ python3                 # Python interpreter
    â””â”€â”€ lib/python3.9/site-packages/
        â”œâ”€â”€ sympy/                  # Symbolic mathematics
        â”œâ”€â”€ numpy/                  # Numerical computing
        â”œâ”€â”€ torch/                  # PyTorch
        â”œâ”€â”€ transformers/           # HuggingFace transformers
        â””â”€â”€ [other packages]
```

---

## ðŸš€ How to Use This Project

### On Mac (Development)

**Activate Python environment:**
```bash
cd /Users/elhoyabembe/Documents/GitHub/Holy_Calculator
source venv/bin/activate
```

**Test LLM locally:**
```bash
cd llama.cpp
./build/bin/llama-cli \
  -m ../models/quantized/deepseek-math-7b-q4km.gguf \
  -p "Solve for x: 2x + 5 = 13" \
  -n 100
```

**Run Python scripts:**
```bash
python3 scripts/download_model.py
```

### Transfer to Raspberry Pi

**What to copy:**
1. `models/quantized/deepseek-math-7b-q4km.gguf` (3.9 GB) â­ ESSENTIAL
2. `scripts/` folder (all Python code)
3. `requirements.txt` (Python dependencies)
4. llama.cpp binaries (rebuild on Pi or cross-compile)

**Transfer methods:**
```bash
# Option 1: USB Drive
cp models/quantized/deepseek-math-7b-q4km.gguf /Volumes/USB_DRIVE/

# Option 2: SCP (one-time WiFi)
scp models/quantized/deepseek-math-7b-q4km.gguf pi@raspberrypi.local:~/

# Option 3: SD Card (before booting Pi)
# Mount SD card and copy directly
```

---

## ðŸ“Š Storage Breakdown

| Location | Size | Keep on Mac? | Transfer to Pi? |
|----------|------|--------------|-----------------|
| `models/base/` | 12.9 GB | âœ… Yes (for re-quantization) | âŒ No |
| `models/quantized/deepseek-math-7b-f16.gguf` | 13 GB | âš ï¸ Optional | âŒ No |
| `models/quantized/deepseek-math-7b-q4km.gguf` | 3.9 GB | âœ… Yes | âœ… **YES!** |
| `models/quantized/deepseek-math-7b-q5km.gguf` | ~4.8 GB | âœ… Yes (backup) | âš ï¸ Optional |
| `llama.cpp/` | ~500 MB | âœ… Yes | âœ… Yes (rebuild) |
| `scripts/` | ~5 MB | âœ… Yes | âœ… Yes |
| `venv/` | ~1 GB | âœ… Yes | âŒ No (rebuild on Pi) |
| **TOTAL** | ~31 GB | | **~4.5 GB** |

---

## ðŸ”‘ Key Files Explained

### For Development (Mac)
- `llama.cpp/build/bin/llama-cli` - Test LLM inference
- `scripts/download_model.py` - Download models from HuggingFace
- `requirements.txt` - Install Python packages (`pip install -r requirements.txt`)
- `venv/` - Isolated Python environment

### For Deployment (Pi)
- `models/quantized/deepseek-math-7b-q4km.gguf` - THE quantized model
- `scripts/cascade/` - (Future) SymPy â†’ Wolfram â†’ LLM logic
- `scripts/hardware/` - (Future) TI-84 interface

### Documentation
- `docs/offline-pi-deployment-plan.md` - How to deploy without WiFi
- `docs/project-status-summary.md` - Current project status
- `docs/phase1-summary.md` - What we've completed

---

## ðŸŽ“ Git Repository

Your project is a Git repository:

```bash
cd /Users/elhoyabembe/Documents/GitHub/Holy_Calculator
git status
git log
```

**Recent commits:**
1. Phase 0: Environment setup
2. Phase 1: Model download complete

---

## âš¡ Quick Commands

### See what's taking up space:
```bash
cd /Users/elhoyabembe/Documents/GitHub/Holy_Calculator
du -sh *
```

### Check model file sizes:
```bash
ls -lh models/quantized/
```

### Test if LLM works:
```bash
cd llama.cpp
./build/bin/llama-cli --version
```

### Update Python requirements:
```bash
source venv/bin/activate
pip freeze > requirements.txt
```

---

## ðŸŒŸ Next Steps

1. **Phase 2 Complete**: Wait for Q5_K_M to finish quantizing (~5 min)
2. **Phase 3-4**: Set up monitoring and testing
3. **Phase 5**: Build SymPy integration (offline math solver)
4. **Phase 6**: Wolfram Alpha integration (optional, needs internet)
5. **Phase 7**: Cascade orchestrator (SymPy â†’ Wolfram â†’ LLM)
6. **Phase 8**: Transfer to Raspberry Pi and test offline!

---

**Last Updated**: November 28, 2025
**Project Status**: Phase 2 nearly complete (90%)
**Ready for Pi?**: Almost! (need to build cascade logic first)
